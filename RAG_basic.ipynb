{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval augmented generation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex,SimpleDirectoryReader\n",
    "documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement FAISS (from versions: none)\n",
      "ERROR: No matching distribution found for FAISS\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\lenovo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install FAISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\New Recordings\\Langchain Videos\\LLAmindex\\Projects\\Llama_index\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 25/25 [00:00<00:00, 247.44it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:03<00:00, 11.11it/s]\n"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x21c577edf90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever=VectorIndexRetriever(index=index,similarity_top_k=4)\n",
    "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "\n",
    "query_engine=RetrieverQueryEngine(retriever=retriever,\n",
    "                                  node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_engine.query(\"What is attention is all yopu need?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: The paper \"Attention Is All You Need\" proposes a new\n",
      "network architecture called the Transformer. This architecture is\n",
      "based solely on attention mechanisms and does not use recurrent or\n",
      "convolutional neural networks. The paper demonstrates that the\n",
      "Transformer models outperform existing models in terms of quality,\n",
      "parallelizability, and training time. The Transformer achieves state-\n",
      "of-the-art results in machine translation tasks and generalizes well\n",
      "to other tasks such as English constituency parsing.\n",
      "______________________________________________________________________\n",
      "Source Node 1/1\n",
      "Node ID: be144ab8-cb0a-44fa-af69-3dbfe555e41a\n",
      "Similarity: 0.8107415810551661\n",
      "Text: Provided proper attribution is provided, Google hereby grants\n",
      "permission to reproduce the tables and figures in this paper solely\n",
      "for use in journalistic or scholarly works. Attention Is All You Need\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google\n",
      "Brain noam@google.comNiki Parmar∗ Google Research\n",
      "nikip@google.comJakob Uszkor...\n",
      "The paper \"Attention Is All You Need\" proposes a new network architecture called the Transformer. This architecture is based solely on attention mechanisms and does not use recurrent or convolutional neural networks. The paper demonstrates that the Transformer models outperform existing models in terms of quality, parallelizability, and training time. The Transformer achieves state-of-the-art results in machine translation tasks and generalizes well to other tasks such as English constituency parsing.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a model architecture that rely entirely on an attention mechanism to draw global dependencies between input and output. They eschew recurrence and do not use sequence-aligned RNNs or convolution. Transformers allow for significantly more parallelization and have been shown to achieve state-of-the-art results in tasks such as translation. They can be trained faster than architectures based on recurrent or convolutional layers.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = faiss.IndexFlatL2(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are transformers?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      5\u001b[0m load_dotenv()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Install FAISS and SentenceTransformer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install faiss-cpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<frozen os>:684\u001b[0m, in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:744\u001b[0m, in \u001b[0;36mcheck_str\u001b[1;34m(value)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "## Retrieval augmented generation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Install FAISS and SentenceTransformer\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load your documents\n",
    "# Assuming SimpleDirectoryReader(\"data\").load_data() returns a list of document texts\n",
    "from llama_index import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "document_texts = [doc['text'] for doc in documents]  # Adjust based on actual structure\n",
    "\n",
    "# Step 2: Convert documents to embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose an appropriate model\n",
    "document_embeddings = model.encode(document_texts)\n",
    "\n",
    "# Step 3: Build the FAISS index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # Using L2 distance\n",
    "index.add(np.array(document_embeddings))\n",
    "\n",
    "# Step 4: Querying the FAISS index\n",
    "class FAISSQueryEngine:\n",
    "    def __init__(self, index, model, documents):\n",
    "        self.index = index\n",
    "        self.model = model\n",
    "        self.documents = documents\n",
    "    \n",
    "    def query(self, query_text, top_k=4, similarity_cutoff=0.80):\n",
    "        query_embedding = self.model.encode([query_text])\n",
    "        distances, indices = self.index.search(np.array(query_embedding), k=top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if distances[0][i] <= similarity_cutoff:\n",
    "                results.append((self.documents[idx], distances[0][i]))\n",
    "        return results\n",
    "\n",
    "query_engine = FAISSQueryEngine(index, model, document_texts)\n",
    "\n",
    "# Example query\n",
    "response = query_engine.query(\"What is attention is all you need?\")\n",
    "for res, dist in response:\n",
    "    print(f\"Document with distance {dist}: {res}\")\n",
    "\n",
    "# Persisting the FAISS index (if needed)\n",
    "faiss.write_index(index, 'faiss_index.bin')\n",
    "\n",
    "# Loading the FAISS index (if needed)\n",
    "# index = faiss.read_index('faiss_index.bin')\n",
    "# query_engine = FAISSQueryEngine(index, model, document_texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
